{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HEFRY ANESTI\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xarray\\coding\\times.py:167: SerializationWarning: Ambiguous reference date string: 850-01-01 0:0:0. The first value is assumed to be the year hence will be padded with zeros to remove the ambiguity (the padded reference date string is: 0850-01-01 0:0:0). To remove this message, remove the ambiguity by padding your reference date strings with zeros.\n",
      "  warnings.warn(warning_msg, SerializationWarning)\n"
     ]
    }
   ],
   "source": [
    "# ### States\n",
    "path_LUH2 = 'D:/kerja/asisten riset/vol/milkunC/achaidir/LUH2 2022/states.nc'\n",
    "luh2_states = xarray.open_dataset(path_LUH2, engine=\"netcdf4\", decode_times=False)\n",
    "luh2_states_worldwide = luh2_states.isel(time=slice(1000, 1173))\n",
    "luh2_states_worldwide['time'] = pd.date_range(start=\"1850-01-01\", end=\"2022-01-01\", freq='YS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "luh2_added_states_worldwide = xarray.open_dataset(\"D:/kerja/asisten riset/vol/milkunC/achaidir/LUH2 2022/multiple-states_input4MIPs_landState_ScenarioMIP_UofMD-IMAGE-ssp119-2-1-f_gn_2015-2100.nc\",\\\n",
    "                                        engine=\"netcdf4\", decode_times=False)\n",
    "luh2_added_trans = luh2_added_states_worldwide.drop_vars(['lat_bounds', 'lon_bounds', 'time_bnds'])\n",
    "luh2_added_trans.coords['time'] = pd.date_range(start='2015-01-01', end='2100-01-01', freq='YS')\n",
    "\n",
    "# ### Static\n",
    "static = 'D:/kerja/asisten riset/vol/milkunC/achaidir/LUH2 2022/staticData_quarterdeg.nc'\n",
    "luh2_static = xarray.open_dataset(static, engine=\"netcdf4\")\n",
    "\n",
    "country_code = pd.read_excel(\"D:/kerja/asisten riset/vol/milkunC/achaidir/LUH2 2022/ISO-3166-Country-Code.xlsx\", engine=\"openpyxl\")\n",
    "ccode_iso = list(country_code['country-code'])\n",
    "cname_iso = list(country_code['name'])\n",
    "\n",
    "ccode_convert = np.zeros((720, 1440), dtype=\"<U64\")\n",
    "\n",
    "ccode_dict = {}\n",
    "for i, ccode in enumerate(ccode_iso):\n",
    "    ccode_dict[ccode] = cname_iso[i]\n",
    "\n",
    "ccode_worldwide_int = luh2_static['ccode'].to_numpy().astype('int64')\n",
    "\n",
    "for i in range(720):\n",
    "    for j in range(1440):\n",
    "        if ccode_worldwide_int[i][j] in ccode_dict.keys():\n",
    "            ccode_convert[i][j] = ccode_dict[ccode_worldwide_int[i][j]]\n",
    "        else:\n",
    "            ccode_convert[i][j] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_class = pd.read_excel(\"D:/kerja/asisten riset/vol/milkunC/achaidir/FAO/Crop Classification_latest.xlsx\", engine=\"openpyxl\", skiprows=1)\n",
    "crop_class = crop_class.drop('Unnamed: 0', axis=1)\n",
    "crop_class.rename(columns={'FAO Crops': 'Item'}, inplace=True\n",
    "                  )\n",
    "fao_stat = pd.read_excel(\"D:/kerja/asisten riset/vol/milkunC/achaidir/FAO/fao_all_data.xlsx\", engine=\"openpyxl\")\n",
    "fao_stat.dropna(axis=0, inplace=True)\n",
    "\n",
    "fao_stat_area_harv = fao_stat[fao_stat['Element']==\"Area harvested\"]\n",
    "fao_stat_area_harv.insert(9, 'True Value', fao_stat_area_harv['Value']/100, True)\n",
    "faostat_area_harvested = fao_stat_area_harv.merge(crop_class, on='Item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfrac = xarray.open_dataset(\"D:/kerja/asisten riset/vol/milkunarc/cadlan/GFRAC_GRAPC_Interpolation/GFRAC-RESAMPLE-NEW-1970-2100.nc\", engine='netcdf4')\n",
    "gfrac_ngfbfc = gfrac.coords['NGFBFC'].data.tolist()\n",
    "\n",
    "gfrac_ngfbfc_ir_rf = []\n",
    "for gfrac_cls in gfrac_ngfbfc:\n",
    "    if  gfrac_cls[0:2] == 'RF' or gfrac_cls[0:2]== 'IR':\n",
    "        gfrac_ngfbfc_ir_rf.append(gfrac_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi interpolasi\n",
    "def grid_level(kolom_sebelumnya, percent):\n",
    "    return np.divide(np.multiply(kolom_sebelumnya, percent), 100)\n",
    "\n",
    "def percent_change(gfrac, kolom_sekarang, kolom_sebelumnya):\n",
    "    if kolom_sebelumnya != 0:\n",
    "        result = np.multiply(np.nan_to_num(np.divide(kolom_sekarang, kolom_sebelumnya)), 100)\n",
    "    else:\n",
    "        result = np.nan\n",
    "    return grid_level(gfrac, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfrac_interp(gfrac_class, lat_idx, country):\n",
    "\n",
    "    arr = np.zeros((56, 1, 1, 1440), dtype=\"float64\")\n",
    "\n",
    "    arr[0] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=0).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    arr[5] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=1).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    arr[10] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=2).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    arr[15] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=3).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    arr[20] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=4).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    arr[25] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=5).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    arr[30] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=6).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    arr[35] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=7).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    arr[40] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=8).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    arr[45] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=9).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    arr[50] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=10).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    arr[55] = np.array(([np.where(country in ccode_convert, gfrac['GFRAC_res'].isel(time=11).sel(NGFBFC=gfrac_class).to_numpy(), 0)[lat_idx]])).sum()\n",
    "    \n",
    "    years = list(np.linspace(1970, 2025, 56, dtype=\"int64\"))\n",
    "    gfrac_any_df = pd.DataFrame()\n",
    "\n",
    "    columns_to_add = [\n",
    "    pd.DataFrame({\n",
    "        f'{year}': arr[i][0][0]\n",
    "        }) for i, year in enumerate(years) \n",
    "    ]\n",
    "    gfrac_any_df = pd.concat([gfrac_any_df] + columns_to_add, axis=1)\n",
    "\n",
    "    gfrac_any_df_copy = gfrac_any_df.copy()\n",
    "    columns_any_df = gfrac_any_df_copy.columns.astype(int)\n",
    "\n",
    "    for column_idx, column in enumerate(columns_any_df[1:]):\n",
    "        if gfrac_class[0:2] == 'RF':\n",
    "            current_year_fao = (faostat_area_harvested[(faostat_area_harvested['Year'] == column)&(faostat_area_harvested['Area'] == country)&\n",
    "                                    (faostat_area_harvested['IMAGE Classification']==gfrac_class.replace('RF ',''))]['True Value'].sum())\n",
    "            \n",
    "            previous_year_fao = (faostat_area_harvested[(faostat_area_harvested['Year'] == column - 1)&(faostat_area_harvested['Area'] == country)&\n",
    "                                            (faostat_area_harvested['IMAGE Classification'] == gfrac_class.replace('RF ',''))]['True Value'].sum())\n",
    "\n",
    "            current_year_gfrac = (gfrac_any_df_copy[f\"{column}\"])\n",
    "            previous_year_gfrac = (gfrac_any_df_copy[f\"{column - 1}\"])\n",
    "\n",
    "            if current_year_gfrac.any() == 0.0 or np.isnan(current_year_gfrac.any()): \n",
    "                gfrac_any_df_copy[f\"{column}\"] = percent_change(previous_year_gfrac, current_year_fao, previous_year_fao)\n",
    "                arr[column_idx+1]  = percent_change(previous_year_gfrac, current_year_fao, previous_year_fao)\n",
    "        \n",
    "        elif gfrac_class[0:2] == 'IR':\n",
    "            current_year_fao = (faostat_area_harvested[(faostat_area_harvested['Year'] == column)&(faostat_area_harvested['Area'] == country)&\n",
    "                                    (faostat_area_harvested['IMAGE Classification']==gfrac_class.replace('IR ',''))]['True Value'].sum())\n",
    "            \n",
    "            previous_year_fao = (faostat_area_harvested[(faostat_area_harvested['Year'] == column - 1)&(faostat_area_harvested['Area'] == country)&\n",
    "                                            (faostat_area_harvested['IMAGE Classification'] == gfrac_class.replace('IR ',''))]['True Value'].sum())\n",
    "            \n",
    "            current_year_gfrac = (gfrac_any_df_copy[f\"{column}\"])\n",
    "            previous_year_gfrac = (gfrac_any_df_copy[f\"{column - 1}\"])\n",
    "            \n",
    "            if current_year_gfrac.any() == 0.0 or np.isnan(current_year_gfrac.any()): \n",
    "                gfrac_any_df_copy[f\"{column}\"] = percent_change(previous_year_gfrac, current_year_fao, previous_year_fao)\n",
    "                arr[column_idx+1]  = percent_change(previous_year_gfrac, current_year_fao, previous_year_fao)\n",
    "\n",
    "    gfrac_any_df_1970_2022 = gfrac_any_df_copy[gfrac_any_df_copy.columns[:-3]]\n",
    "\n",
    "    arr_interp_carstock_any = []\n",
    "    \n",
    "    years_1970_2022 = list(np.linspace(1970, 2022, 53, dtype=\"int64\"))\n",
    "    \n",
    "    for year in years_1970_2022:\n",
    "        arr_interp_carstock_any.append([[gfrac_any_df_1970_2022[f'{year}']]])\n",
    "\n",
    "    return np.array(arr_interp_carstock_any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_interp2arr(gfrac_class, country):\n",
    "    \n",
    "    interp_any = [] \n",
    "    for lat in range(720):\n",
    "        interp_any.append(gfrac_interp(gfrac_class=f'{gfrac_class}', lat_idx=lat, country=country))\n",
    "        \n",
    "    arr_interp_any = np.array(interp_any)\n",
    "    gfrac_interp_any_year = [] \n",
    "    \n",
    "    for year in range(53):\n",
    "        gfrac_interp_any_year.append([])\n",
    "        for i in range(len(arr_interp_any)):\n",
    "            gfrac_interp_any_year[year].append(arr_interp_any[i][year])\n",
    "\n",
    "    gfrac_interp_any_year = np.array(gfrac_interp_any_year).reshape(53, 720, 1440)\n",
    "\n",
    "    return gfrac_interp_any_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldwide_gfrac_interp = []\n",
    "for cty_idx, country in enumerate(list(np.unique(ccode_convert))):\n",
    "    worldwide_gfrac_interp.append([])\n",
    "    for gfrac_class in gfrac_ngfbfc_ir_rf:\n",
    "        crops_interp_all_expand = np.expand_dims(convert_interp2arr(gfrac_class, country), axis=-1)\n",
    "        worldwide_gfrac_interp[cty_idx].append(crops_interp_all_expand)\n",
    "\n",
    "worldwide_gfrac_interp_arr = np.concatenate(worldwide_gfrac_interp, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfrac_interp_netcdf = xarray.Dataset({\n",
    "        \"gfrac_interp\":([\"country\", \"time\", \"lat\", \"lon\", \"ngfbfc\"], worldwide_gfrac_interp_arr)\n",
    "    },\n",
    "    coords={\n",
    "        \"NGFBFC\": np.array(gfrac_ngfbfc_ir_rf, dtype='<U35'),\n",
    "        \"country\": list(np.unique(ccode_convert)),\n",
    "        \"time\":pd.date_range(\"1970-01-01\", \"2022-01-01\", freq='YS'),\n",
    "        \"lon\":luh2_states_worldwide.coords[\"lon\"].to_numpy(),\n",
    "        \"lat\":luh2_states_worldwide.coords[\"lat\"].to_numpy()\n",
    "    })\n",
    "\n",
    "gfrac_interp_netcdf.to_netcdf(\"c:/Users/HEFRY ANESTI/Downloads/GFRAC-INTERPOLATION-WORLDWIDE-1970_2022.nc\", mode='w', format=\"NETCDF4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
